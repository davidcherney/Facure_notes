{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "467dc273-a99a-4610-a5d5-7785c4bee488",
   "metadata": {},
   "source": [
    "III Effect Heterogeneity and Personalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e750c5-9a2e-4d39-9398-852163e955a6",
   "metadata": {},
   "source": [
    "# 6 Effect Heterogeneity \n",
    "\n",
    "Which \"unit\" should you treat when treatment effects different people differently? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c1b8f-68c9-4f0a-b01e-a92e1150fbd9",
   "metadata": {},
   "source": [
    "## Why Prediction Is Not the Answer \n",
    "\n",
    "**Definition:** The <u>individual treatment effect</u> (ITE) of indivicual $i$ is $\\widehat{\\frac{\\Delta Y_i}{\\Delta T}}$. \n",
    "\n",
    "\n",
    "We want to group individuals by their  ITE, but this quantity is not observable for an individual since it is defined in terms of counterfactuals. Thus it is not possible to have labeled data with ITE for each individual.\n",
    "\n",
    "To estimate IET we assume that units with the same $X$ values have the same ITE; we plot $Y$ vs $T$ with a bunch of data points colored by their $X$ values and find a different slope for each group of $X$ values. \n",
    "\n",
    "**Definition:** The <u>conditional average treatment effect</u> (CATE) for $\\{i|X_i =x\\}$ is $\\frac{\\partial }{\\partial t} E[Y|X=x]$. \n",
    "\n",
    "<img src=\"images/slope_segmentation.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a6ed0-6c37-454b-b48f-4ebd1bf7b50d",
   "metadata": {},
   "source": [
    "## CATE with Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b291f0-afd0-4e87-a086-ed0c5f678594",
   "metadata": {},
   "source": [
    "Instead of a model of the form \n",
    "$$\n",
    "y_i = β_0 + β_1t_i + β_2X_i + e_i\n",
    "$$\n",
    "we will use a model (with an interaction term) of the form\n",
    "$$\n",
    "y_i = β_0 + β_1t_i + β_2X_i + β_3t_iX_i + e_i\n",
    "$$\n",
    "so that \n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial t} = \\beta_1 +\\beta_3 X_i\n",
    "$$\n",
    "Thus,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d378196-12ca-4cfb-9715-f3a9e808904a",
   "metadata": {},
   "source": [
    "- $\\hat{y}(x)$ is the model for outcome\n",
    "- $\\widehat{\\frac{\\partial y}{\\partial t}}(x)$ is the model for CITE. \n",
    "- $\\beta_3$ is rate of change of the effect with $X$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50699999-8feb-4688-8646-0fc259ef2db6",
   "metadata": {},
   "source": [
    "e.g. Let the treatment $t$ be discount and the effect $S$ be sales. We seek\n",
    "$$\n",
    "\\frac{\\partial }{\\partial t} E[S|X=x]\n",
    "$$\n",
    "\n",
    "We will assume a randomized treatment $S_0,S_1 \\perp T|X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66bf1a-5558-4400-956b-7755da5eed0b",
   "metadata": {},
   "source": [
    "In patsy formula syntax, the operator \\* makes several of these terms:\n",
    "e.g. a\\*b will include the terms $a$, $b$ and $a b$ in your regression. \n",
    "\n",
    "```python\n",
    "import statsmodels.formula.api as smf\n",
    "            X = [\"C(month)\", \"C(weekday)\", \"is_holiday\", \"competitors_price\"]\n",
    "            regr_cate = smf.ols(f\"sales ~ discounts*({'+'.join(X)})\",\n",
    "                                data=data).fit()\n",
    "```\n",
    "\n",
    "If you only want the multiplicative term, you can use the : operator inside the formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99287ce3-72b1-4710-bae3-b1e939807a20",
   "metadata": {},
   "source": [
    "You *can* get the CATE function $\\widehat{\\frac{\\partial y}{\\partial t}} = \\hat{\\beta}_1 + \\hat{\\beta}_3 X$ from\n",
    "1. The coefficients $\\hat{\\beta_1},~ \\hat{\\beta}_3  $ from the fit\n",
    "2. the definition of the derivative with $\\epsilon \\to 0$\n",
    "3. using the linearity of $\\hat{y}$ to take advantage of the fact that \n",
    "$\\hat{\\frac{\\partial y}{\\partial t}} = \\hat{y}(t+1) - \\hat{y}(t)$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2845c3fd-1428-4d6b-86f8-2fc7d85a3e23",
   "metadata": {},
   "source": [
    "**Definition:** In <u>price discrimination</u> firms  discriminate consumers into those who are willing to pay more and charge them more.\n",
    "\n",
    "e.g. half-price entry tickets for students. In this case, the company knows that students make less money on average, meaning they have less to spend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec427b2d-0be6-4d57-9727-e85c8b2afac0",
   "metadata": {},
   "source": [
    "**Definition:** in <u>intertemporal price discrimination</u> a company distinguish the price sensitivity of customers based on time. \n",
    "\n",
    "For exaple airline tickets bought just before the flight cost much more than those a few months early. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c29a3-ede5-438d-851d-9c1da8a200d5",
   "metadata": {},
   "source": [
    "## Effect by Model Quantile \n",
    "\n",
    "It would be very useful if you could somehow order units by ITE. WE can order them by CATE and hope that this gives a similar ordering. \n",
    "\n",
    "1. From predicted $\\text{CATE}(x_i)$, calculate quantile intervals $Q_k$\n",
    "2. label each unit $i$ by the middle $m_k$ of the quantile interval $Q_k$ it's $\\text{CATE}(x_i)$ is in.\n",
    "3. You now have groups $G_k = \\{i|\\widehat{\\text{CATE}}(x_i) \\in Q_k\\}$ for each quantile labeled by $m_k$\n",
    "3. For each quantile group $G_k$ calculate $\\hat{\\beta}_1$ from the regression $y=\\beta_0+\\beta_1 t$ over points in $G_k$\n",
    "    - that is $\\hat{\\beta}_{1,k} = \\frac{\\sum_{i\\in G_k}(t_i - \\bar{t}) y_i}{\\sum_{G_k}(t_i - \\bar{t})^2}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae5432c-2ed5-4ae5-911f-4a3c0de49557",
   "metadata": {},
   "source": [
    "<img src=\"images/CATE_quantile_vs_id.png\" width=\"300\"> <img src=\"images/CATE_quantile_bar.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297f1c4f-c564-4a51-b6a4-b0bf57985dd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "--- \n",
    "Me: This seems redundant. \n",
    "1. We calculate CATE for each x value. \n",
    "2. We partition by CATE quantile.\n",
    "3. we calculate CATE for each quantile. \n",
    "\n",
    "There are two reasons I can see for doing this; \n",
    "1. the set of units in a quantile group $G_k$ is going to be bigger than the set of units in a group for a single value of $X$. So, fitting on the bigger group gives the ATE on the larger group in a natural way in the sense of regression, as opposed to an artificial weighted average of CATEs over $X$ values. \n",
    "2. A sanity check of some kind... on \n",
    "\n",
    "- the $y=\\beta_0 + \\beta_1t+\\beta_2tX$ model with derivative $\\frac{ \\partial y}{\\partial t} = \\beta_1+\\beta_2 X$, which you might not believe in so we compare it to \n",
    "- the $y=\\beta_0+\\tau t$ model, which we believe from earlier on one quantile.\n",
    "\n",
    "at any rate, when ordering by CATE quantile and then plotting CATE, we expect nearly y=x behavior. \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ece455-2145-42cb-abe0-319827ab954b",
   "metadata": {},
   "source": [
    "## Cumulative Effect\n",
    "\n",
    "In the previous section \n",
    "- we formed groups $G_k$ from quantiles of predicted ITE, $\\widehat{\\frac{ \\partial y}{\\partial t} }(x)= \\hat{\\beta}_1+\\hat{\\beta}_2 x$ \n",
    "- we fit $y=\\beta_0+\\beta_1 t$ over points in $G_k$ to obtain the ATE  $\\beta_{1,k}$ of group $G_K$.\n",
    "\n",
    "Here, \n",
    "- accumulate one group on top of the other to form the sequence $H_k = \\underset{k'\\leq k}{\\cup} G_k$. \n",
    "- Fit $y=\\delta_0+\\delta_1 t$ over points in $H_k$ to obtain the ATE  $\\delta_{1,k}$ of cumulative group $H_k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11a741-0db4-43df-925e-fc420329419a",
   "metadata": {},
   "source": [
    "<img src=\"images/cumulative_effect_curve.png\" width=\"300\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6abdc7-b9f9-4336-abf9-31b20fb64031",
   "metadata": {},
   "source": [
    "The grouping criteria is up to you; here we group by the top predicted ITE but you might decide to order or group by some other criteria. (An example would be great, but alas.) If your grouping does a good job of sorting the CATE then the resulting curve will gradually approach the average effect.\n",
    "\n",
    "The goal is to find groups that have ATE well above average ATE. Thus, you might want to take the area between the cumulateive efect curve and the average ATE over the whle population\n",
    "\n",
    "The groups on the left are very small and thus have very high variance. \n",
    "\n",
    "Thre is a solution to this; weigh the cumulative outcome by the cumulative population fraction to obtain cumulative gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a91688e-52d3-4d5a-88a8-675b92a7ae67",
   "metadata": {},
   "source": [
    "## Cumulative Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764c195a-06c0-4d2e-abd9-b222bef295f1",
   "metadata": {},
   "source": [
    "Let $N_{\\text{cum} , k } = | H_k |$. \n",
    "\n",
    "Multiply each point in the cumulative efect curve by $\\frac{N_{\\text{cum},k}}{N}$. \n",
    "\n",
    "<img src=\"images/cumulative_gain.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead3754-4ceb-435c-bc58-31e6f85fd1bf",
   "metadata": {},
   "source": [
    "Subtracting ave average CATE before applying this factor give the \"normalized\" cumulative gain curve wherein the area of interest is more readily seen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8283d718-086a-4d86-9a9a-7e3b0a298d6c",
   "metadata": {},
   "source": [
    "The area between is the same for both curves, and can be thought of as an indicator of a good ordering of the units; ordering so that we can see the level of effect above random. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab557c1-3834-40ee-9d28-62e5a284e8cc",
   "metadata": {},
   "source": [
    "Evaluation of causal models is an area of research that is still developing and has many blindspots. E.g., how do we check for correct CATE prediction instead of this check for a good ordering ? We have no analog of R^2 or MSE. \n",
    "\n",
    "One thing we do have is Target Transformation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb6cc7b-1383-4ac5-9ba0-a45a48bb9c70",
   "metadata": {},
   "source": [
    "## Target Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d6446b-425a-4b8f-9074-83aa9f4d7285",
   "metadata": {},
   "source": [
    "Using an unbiased estimator of $\\tau$ to estimate MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a5925e-1faa-480d-8d7f-e98b8b1bc40d",
   "metadata": {},
   "source": [
    "## Binary Outcomes\n",
    "\n",
    "$E[ Y |T=t]$ vs $t$ has an S shape, possibly heavy in the top or bottom. \n",
    "\n",
    "<img src=\"images/binary_outcome_shapes.png\" width=\"500\">\n",
    "\n",
    "For us, likely, most customers will be non-switchers, so we will be bottom heavy.\n",
    "\n",
    "Intuitively, the closer to the middle of the S shape (the tipping point, $E[Y|T=t]^{-1}(0.50)$) the higher the effect will be since the slope is largest there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3917eaff-08da-4501-b769-5422381a23f3",
   "metadata": {},
   "source": [
    "<a href src=\"https://dl.acm.org/doi/abs/10.5555/3586589.3586648\">ends up</a> these models can do very well in some circumstances. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3531bd-3dd8-4c2d-ac1b-e9078ca6ed7b",
   "metadata": {},
   "source": [
    "## CATE for Decision Making\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbf06f4-51f9-4a23-b4ef-9b15e28084dc",
   "metadata": {},
   "source": [
    "# 7 Metalearners \n",
    "\n",
    "Metalearners are an effortless way to leverage off-the-shelf predictive machine learning algorithms for approximating treatment effects.\n",
    "\n",
    "All predictive models, such as linear regression, boosted decision trees, neural networks, or Gaussian processes, can be repurposed for causal inference using the approaches described in this chapter\n",
    "\n",
    "At the time of this writing (2023), all the causal inference packages for python are in their early stage. They contain metalearners that are slow because they are running too much python which is slow. They are thus not appropriate for large data applications. Thus, it is not a good idea for a book like this to promote a causal inverence python package at this time.\n",
    "\n",
    "We will build our metalearners from the ground up. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcb1aab-4fae-4ef6-af71-a73025f4c03a",
   "metadata": {},
   "source": [
    "## T-Learner \n",
    "\n",
    "The T is for two as in the binary treatment case. There are two models to create, $\\{ \\mu_t(x) | t\\in\\{1,0\\}\\} $.  \n",
    "\n",
    "$$\n",
    "\\mu_0 (x)=E[Y|T=0, X=x]\\\\\n",
    "\\mu_1 (x)=E[Y|T=1, X=x]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d0495-2fed-44da-bdd2-4ea8286eaafb",
   "metadata": {},
   "source": [
    "and of course\n",
    "$$\n",
    "\\hat{\\tau}(x) = \n",
    "\\hat{\\mu}_1(x) -\\hat{\\mu}_0(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4257a2d-2853-4fbf-a359-21be83acfaad",
   "metadata": {},
   "source": [
    "Me: I'm a little annoyoed that we do not call these $\\hat{Y}_0(x)$ and $\\hat{Y}_1(x)$. I really thing that conflating expectation values with models is getting something wrong. \n",
    "\n",
    "For example, you can use boosted tree regression for both. However, tree regessors self-regulate; when there are fewer points to train on there are fewer pieces of piecewise function. This means that when the treated and untreated groups are of very different sizes, say 25 treated and 10,000 untreated, one of these functions will be able to pick up non-linearities that the other isn't, and in those regions of $x$ values there is bias in $\\hat{\\tau}(x)$. The X-learner helps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150a0f30-7547-480a-9c7d-a692065e0e01",
   "metadata": {},
   "source": [
    "Generalizing beyond the binary treatment case, you can build T learners for T categorical with any number of categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444c767-418d-4fb7-8297-ab886ed44ede",
   "metadata": {},
   "source": [
    "## X-Learner\n",
    "\n",
    "This is hard to understand, expect to need a re-read. \n",
    "\n",
    "In the case where one treatment group is much larger than the other X-learners do much better than T-learners. This is because they correct for inaccurate imputed potential values in the under-represented treatment group. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b4536-7d7a-496a-9e56-7377b27b8e05",
   "metadata": {},
   "source": [
    "An X-learner has two stages a propensity score model, and it's own model. \n",
    "\n",
    "1. The first stage is a T-learner, $\\mu_0,\\mu_1$. \n",
    "    - use these to impute all missing potential outcome  values $$\\hat{\\mu}_1(X,T=0)\\\\\n",
    "    \\hat{\\mu}_0(X,T=1)$$\n",
    "    - from the imputed potential outcomes calculate TUT and TT estimates\n",
    "$$\n",
    "\\hat{\\tau}(X,T=0) := \\widehat{\\text{TUT}} =  \\hat{\\mu}_1(X,T=0) - Y_{T=0}\\\\\n",
    "\\hat{\\tau}(X,T=1) := \\widehat{\\text{TT}} = Y_{T=1} - \\hat{\\mu}_0(X,T=1) \n",
    "$$\n",
    "2. second stage \n",
    "    - create two more ML models of TUT and TT and train them to predict the $\\tau$s above. We call these second stage models. \n",
    "$$\n",
    "\\hat{\\mu}_{\\tau 1} \\approx TT = E[\\tau(X) | T=0]\\\\\n",
    "\\hat{\\mu}_{\\tau 0} \\approx TUT  = E[\\tau(X) | T=1] \\\\\n",
    "$$\n",
    "3. propensity score model: $\\hat{e}(x) \\approx  P[T|X=x]$.\n",
    "4. the X-learner model $\\hat{\\tau}(x)$ is a propensity score weighted combination of the second stage models:\n",
    "$$\n",
    "\\hat{\\tau}(x) = \\hat{\\mu}_{\\tau 0}(X)\\hat{e}(x) + \\hat{\\mu}_{\\tau 1}(X) (1-\\hat{e} (x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cee234-1123-46ef-bb5d-8ca37a6b5968",
   "metadata": {},
   "source": [
    "Why do we need all this? \n",
    "\n",
    "In our example, \n",
    "- $\\hat{μ}_1$  was fitted on a very small sample and so imputes inaccurately. \n",
    "    - Therefore $\\hat{τ}( X, T = 0) \\sim TUT$ is inacurate. \n",
    "- $\\hat{\\mu}_0$ was accurate  \n",
    "    - therefore $\\hat{\\tau}(X,T=1) \\sim TT $ is accurate. \n",
    "\n",
    "We want to combine the $\\hat{\\tau}$s in a way that give more weight to the accurate model. The propensity score model helps sort out where the two models are accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9f8bd8-1c27-46af-ad9b-411a6f847354",
   "metadata": {},
   "source": [
    "Thinking through it \n",
    "- In places where the probability of treatment is low the TT provides the estimate.\n",
    "- in places where the probability of treatment is high the TUT provides the estimate.\n",
    "\n",
    "In our example, there were very few treated units, for all $x$ probability of treatment is low, and so \n",
    "- TT provides the majority of the estimate, and \n",
    "- TUT plays a minor role. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0419abda-1387-47ad-ac88-51e3fa03ce19",
   "metadata": {},
   "source": [
    "## Metalearners for Continuous Treatments \n",
    "The thing about continuous treatments is\n",
    "- there are infinitely many values $t$ for $T$ \n",
    "- for each $i$ only once outcome $Y(t)$ can be observed,\n",
    "- treatment effect is based on thoses non-obersvables.\n",
    "\n",
    "So, the goal is to impute all the  unobserved outcomes for for each $i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c627fb17-c06b-4ebc-bfc6-75db47eb6f99",
   "metadata": {},
   "source": [
    "### S-Learner\n",
    "\n",
    "This is the simplest learner there is. \n",
    "\n",
    "A single (hence the S) machine learning model $μ_s$ is trained on $X$ and $T$ to estimate $Y$ and obtain the estimate\n",
    "$$\n",
    "\\hat{\\mu}_s(x,t) = \\mu(x) = E[Y|T=t,X=x]\n",
    "$$\n",
    "which consists of almost everywhere counterfactual potential outcome predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352fdb1b-dc6f-4c76-ab15-22c71354735d",
   "metadata": {},
   "source": [
    "After you train the model... \n",
    "\n",
    "To put these counterfactual predicitons into a DataFrame requires a bit of tabular data trickery; \n",
    "- Partition the continuou of $T$ values into $n$ parts $t_k$. \n",
    "- for each row in the original df\n",
    "    - replicate the row once for each part\n",
    "- add a new column and fill it with prediciton values $\\hat{\\mu}_s(x,t)$ \n",
    "\n",
    "For a fixed $i$ you can then plot n points of $Y_i(t)$.\n",
    "You can also calculate the regression slope of each, the ITE for each $i$, if you want. \n",
    "\n",
    "Now you can order units by predicted effect quantile, average their estimated ITE over their quantile for an estimated ATE(>p), make a normalized cumulitive gain curve, and use the area under it as an metric of how well you put the units in order.  \n",
    "(as in ch6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b579a8c-aeb1-4ee9-85dc-fd1632366490",
   "metadata": {},
   "source": [
    "S-learner is a good first bet for any causal problem, \n",
    "- It also tends to perform OK, even if it doesn’t have random data to train.\n",
    "- it  supports binary,  discrete numerical,   and continuous treatment, \n",
    "\n",
    "Downside: \n",
    "- S-learner is that it tends to bias the treatment effect toward zero (as opposed too up or down); model regularization can restrict the estimated treatment effect. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a38354-01de-450b-ac49-2363d7cdbe24",
   "metadata": {},
   "source": [
    "<img src=\"images/S_learner_bias.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a92d7-639c-42f6-9692-4a807a954096",
   "metadata": {},
   "source": [
    "A way around this, proposed in the same paper by Chernozhukov et al., is Double/Debiased Machine Learning, or the R-learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d415daa-eb26-45ca-b730-114af7f6eeed",
   "metadata": {},
   "source": [
    "## Double/Debiased Machine Learning\n",
    "\n",
    "aka the R-learner..... I'll revisit if needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
