{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4950c7-fc5d-4051-988d-9730735edfdd",
   "metadata": {},
   "source": [
    "Part II: Adjusting for bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa3d1c-1c39-415f-8f55-c5a6840af77a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4 Linear Regression\n",
    "\n",
    "**Definition:** A linear regression is <u>bivariate</u> if there is one independent variable and one dependent variable; $Y=\\tau T +\\beta_0$.\n",
    "\n",
    "Seems like monovariate is another name for this. Trivariate seems to refer to three independent and one dependent. So, just be aware of different authors' conventions. \n",
    "\n",
    "## Regression in A/B Tests \n",
    "\n",
    "The promise; standard error, confidence intervals and any other inference stats for come for free when you use linear regression to calculate ATE. \n",
    "\n",
    "e.g. A streaming service is trying to maximize `watch_time`. It has A/B tested a new recommender system $\\text{challenger}$ on 1/3 of customers $\\{i|\\text{challenger}_i = 1\\}$.  To measure the ATE you perform a fit of\n",
    "$$\n",
    "\\text{watch_time} = \\beta_0 +\\beta_1 \\text{challenger} + e\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c54bb6-8038-4ad7-a61c-7cee4180fa54",
   "metadata": {},
   "source": [
    "Interpretation;\n",
    "- $\\hat{\\beta}_0$ is average time spent watching with old recommender system\n",
    "- $\\hat{\\beta_0}+\\hat{\\beta}_1$ is time spent watching with new \n",
    "- $\\hat{\\beta}_1$ is estimated ATE.\n",
    "\n",
    "We well therefore call $\\tau:=\\beta_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab6c17c-23cc-4155-bd22-9ad34c244514",
   "metadata": {},
   "source": [
    "---\n",
    "## t Test For Regression Coefficients\n",
    "\n",
    "Aside on $t$ statistic for regression coefficient $\\beta_1$ in $y=\\beta_1 + \\beta_0 +e$:\n",
    "\n",
    "Q: What does it mean for a regression coefficient  $\\beta_i$ to have a mean or SE?\n",
    "\n",
    "A:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94cb844-bcf8-48fd-a158-bdc7f7681601",
   "metadata": {},
   "source": [
    "First, center $X$ so we don't have to work with $\\beta_0$. \n",
    "\n",
    "If the data matrix for $x$ is $X$ then the OLS fit to $y= \\beta_1 x +  e$ is\n",
    "$$\\hat{\\beta_1} = (X^T X)^{-1} X^T Y:= AY.$$\n",
    "Therefore\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mathbb{V}(\\hat{\\beta}_1) &= A \\mathbb{V}(Y)A^T \\\\\n",
    "&=(X^T X)^{-1} X^T \\mathbb{V}(Y) X (X^T X)^{-T}  \\\\\n",
    "&= \\mathbb{V}(Y)  (X^T X)^{-1} (X^T X) (X^T X)^{-T}  \\\\\n",
    "&= \\mathbb{V}(Y)  (X^T X)^{-T}  \\\\\n",
    "&= \\frac{ \\mathbb{V}(Y)}{(X^T X)}  \\\\\n",
    "&\\stackrel{\\text{centered}}{=} \\frac{ \\mathbb{V}(Y)}{(n-1)\\mathbb{V}(X)}  \\\\\n",
    "&= \\frac{ \\text{SE}_Y^2}{\\sigma^2_X}  \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Note that the $1$ in there comes from the fact that we have a single independent variable in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edec0b2-66a1-4d88-a168-de37a7b07999",
   "metadata": {},
   "source": [
    "Q: What is a $t$ test for the slope?  \n",
    "\n",
    "A: The most common null hypothesis is that the actual slope is zero. In causal inference this is the null hypothsis that $\\text{ATE}=0$. Then \n",
    "$$\n",
    "\\hat t=\\frac{\\hat{\\beta}_1}{SE_{\\beta} }\n",
    "= \\frac{\\hat{\\beta}_1}{\\frac{\\sigma_{\\beta_1}}{\\sqrt{n} }}\n",
    "$$\n",
    "Since $\\hat{\\beta}_1$ is the estimated ATE, both a large $\\hat{\\beta}_1$ and a small $\\text{SE}$ signal a large value of $t$ and thus statistical significance. If the 95% CI $(\\hat{\\beta}_1 - 1.96 \\text{SE}, \\hat{\\beta}_1+1.96 \\text{SE})$ does not contain the null hypothesis value $0$ then the null hypothesis is rejected with \n",
    "- significance level $\\alpha=5\\%$\n",
    "- confidence level $1- \\alpha = 95\\%$. \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f5f65e-093b-4e55-811c-39e13dde8520",
   "metadata": {},
   "source": [
    "```python\n",
    "In[]: statsmodels.formula.api.smf.ols(formula = 'Y ~ T', data= df) \n",
    "```\n",
    "indeed gives\n",
    "- estimated coefficients $\\beta_0,\\beta_1$. \n",
    "- SE for them\n",
    "- $\\hat{t}$ \n",
    "- $P>|\\hat{t}|$, which I take to be $\\mathbb{P}(t>\\hat{t} ) = 1-\\text{CDF}_t(\\hat{t})$ (Under the null hypothesis) \n",
    "- a two sided 95% CI ... for $\\beta_i$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d19d8-92a1-4517-ad32-3d6ec32adbb6",
   "metadata": {},
   "source": [
    "## Adjusting for Counfounding with Regression\n",
    "\n",
    "In modeling the rate $\\beta_1$ of default on credit line $D$ with respect to credit limit $L$ via\n",
    "$$\n",
    "D = \\beta_0 + \\beta_1 L + e\n",
    "$$\n",
    "there is confounding bias; credit is extended to those who are deemed not a risk based on other factors. Those factors are thus confounding variables $X$ (like wage, credit score, etc). Thus $\\hat{\\beta}_1$ is a biased estimator of $\\text{ATE}$.\n",
    "\n",
    "Finding $\\tau$ in \n",
    "$$\n",
    "D = \\beta_0 + \\tau L +\\theta X+ e\n",
    "$$\n",
    "gives the  rate of default holding confounders constant, removing confounding bias in $\\beta_1$.\n",
    "\n",
    "**Definition:** When you want to find a regression coefficient, and need to add other coefficients to remove confounding bias, the other coefficients are <u>nuisance parameters</u>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc25251-f6e0-47d4-b625-873abbf44e77",
   "metadata": {},
   "source": [
    "## Coefficients as Covariance\n",
    "The OLS estimate of $\\tau$ in \n",
    "$$\n",
    "Y= \\tau T +e\n",
    "$$ is\n",
    "$$\n",
    "\\hat{\\tau} = \\frac{\\text{Cov}(Y,T)}{\\text{Var}(T)}\n",
    "=\\frac\n",
    "{\\mathbb{E}[(T-\\bar{T})(Y-\\bar{Y})]}\n",
    "{\\mathbb{E} [ (T-\\bar{T})^2]}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad43e9fd-ceb7-4e18-9122-909bd61f9297",
   "metadata": {},
   "source": [
    "Regression is a quantification of how $T$ and $Y$ move together, and rescales by units of treatment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44029a47-8c36-4516-bfa5-cd5581aff89d",
   "metadata": {},
   "source": [
    "Me: Note the similarity to sying that $X$ is centered and then saying\n",
    "$$\n",
    "\\hat{\\beta}=(X^TX)^{-1} X^TY. \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02af147d-6600-4a65-afc5-64ce7fc990ee",
   "metadata": {},
   "source": [
    "## Frisch-Waugh-Lovell \n",
    "(FWL)\n",
    "\n",
    "### Building intuition:\n",
    "\n",
    "There is an enlightening way to estimate $\\tau$ in \n",
    "$$y=\\beta_0 +\\tau T + \\beta X +u.$$\n",
    "Fit $T = \\delta_1 X +\\delta_0$ and obtain the residuals $\\tilde{T_i}$. Then\n",
    "$$\n",
    "\\hat{\\tau} = \\frac{\\text{Cov}(Y,\\tilde{T})}{\\text{Var}(\\tilde{T})}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471efe6-4368-4670-8917-5f7d070a1de5",
   "metadata": {},
   "source": [
    "Thus, you can account for the effect on $Y$ of other variables $X$ in the model besides $T$ by \n",
    "1. fitting $T$ as a function of those other variables, and then \n",
    "2. fitting $Y=\\tau \\tilde{T} + \\delta_0$ to obtain $\\hat{\\tau}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7787413-6aa3-41ca-937a-e8df766d76ce",
   "metadata": {},
   "source": [
    "\n",
    "That is, $\\tau$ is the the bivariate coefficient of $T$ after accounting for all other variables to predict $T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d6ef48-3ff1-495e-a3a2-997d59ae72f4",
   "metadata": {},
   "source": [
    "Intuition:  \n",
    "- If you can predict $T$ using $X$ then $T$ is not random WRT $X$. \n",
    "- Residuals $\\tilde{T}$ are random WRT $X$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf4c9f9-9d15-40ba-9f25-f941f58bbcc6",
   "metadata": {},
   "source": [
    "Here is my visualization. I attempt to show how confounding bias, and how not correcting for it, can give the opposite sign ATE of the true ATE when $X$ and $T$ are not independent.\n",
    "\n",
    "<img src=\"images/IMG_6899.jpg\" width=\"400\">\n",
    "\n",
    "(that picture shows the debiasing step but not the denoising step from FWL below) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a400b29b-f2bc-4b47-a3ef-362aea1f5555",
   "metadata": {},
   "source": [
    "The above sets up intuition for the following \n",
    "### FWL Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a56a408-fd09-4373-91ba-b3ac195dd632",
   "metadata": {},
   "source": [
    "**Theorem** (Frisch–Waugh–Lovell): \n",
    "Fitting $Y= \\tau T + \\beta_1 X +\\beta_0 +e$ yields the same $\\hat{\\tau}$ as performing three steps:\n",
    "1. A debiasing step: regress T on X and obtain residuals $\\tilde{T} := T − \\hat{T}$\n",
    "2. A denoising step: regress Y on X and obtain residuals $\\tilde{Y} = Y − \\hat{Y}$\n",
    "3. Outcome model: regress the $\\tilde{Y}$ on residual $\\tilde{T}$ to obtain an estimate $\\hat{\\tau}$ for the causal effect $\\tau$ of $T$ on $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ff043c-662c-458f-839d-f0478d509b84",
   "metadata": {},
   "source": [
    "The standard error of the estimator of the regression parameter is obtainable from the residuals in the first two steps; (seems like this should be called a theorem)\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mathbb{V}(\\hat{\\tau}) \n",
    "&= \\frac{ \\text{SE}_Y^2}{\\sigma^2_X}  \\\\\n",
    "&= \\left(\\frac{\\sigma(\\tilde{Y})} { \\sigma(\\tilde{T}) \\sqrt{n-\\text{DF}}} \\right)^2\n",
    "\\end{array}\n",
    "$$\n",
    "where $n$ is the number of points and DF is the number of parameters estimated by the model including the intercept $\\beta_0$. (so $shape(\\tau)+shape(\\beta_1)+shape(\\beta_0)$, but the noise is not a parameter.)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c8f7a-dcbf-4ee1-b7e8-f26d5f804c29",
   "metadata": {},
   "source": [
    "Lets unfold this\n",
    "- numerator: The more consistant the estimate of $Y$ from $X$ the smaller the uncertainty in $\\hat\\tau$. \n",
    "- denominator: \n",
    "    - the less consistant the estimate of $T$ from $X$ the smaller $SE(\\tau)$ (because $X$ is worse at predicting $T$). Their wording: When treatment varies a lot it is easier to measure the impact of treatment. But... variation of treatment is not what appears here. \n",
    "    - 4 times more points gives about half the variance of $\\hat{\\tau}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ae5be9-597f-404c-ac34-94920c9fcb49",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e1bd5-b081-4030-9780-6360fdc91c13",
   "metadata": {},
   "source": [
    "Their diagram is pretty sexy; \n",
    "\n",
    "#### Debiasing \n",
    "is constructing the fit\n",
    "$T = a X +b$\n",
    "and then applying the transformation \n",
    "$$\n",
    "\\left[\\begin{array}{c} X \\\\ \\tilde{T} \\end{array} \\right]\n",
    "= \\left[\\begin{array}{cc} 1 & 0 \\\\ -a &1 \\end{array} \\right]\n",
    "\\left[ \\begin{array}{c} X \\\\ T\\end{array}  \\right]\n",
    "+\\left[  \\begin{array}{c} 0 \\\\ -b \\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a4783-eb1a-451f-b67f-9bda3d6c87b7",
   "metadata": {},
   "source": [
    "For just the data set pictured below, This is exactly finding the average $T$ value at each $X$ value, and then \n",
    "- translating along the T axis by $b$\n",
    "- shearing in the $\\tilde{T},X$ plane along the $T$ axis to bring the averages to zero. \n",
    "\n",
    "For more general data sets you can't say \"average T value at each y\" but you certainly can say \"average T value\" and after the translation and shearing the data will have the $\\tilde{T}=0$ axis as the line of best fit. \n",
    "\n",
    "That shearing in the $X,T$ plane can not be seen in the $T,Y$ plane other than translation that varies by color. In their image, the average T value at each X value is brought to zero because the centroids laid on a line to begin with. Note that one can find $\\hat{\\tau}$ after debiasing, but one deals with a lot of noise, as seen in the vertical variation. \n",
    "\n",
    "In the loftmost image the black line is line of best fit. In the other two images the black lines are average T values for each $X$ value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ef0ca-74f5-4495-af16-163b87e3a90b",
   "metadata": {},
   "source": [
    "<img src=\"images/debias.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0a6ec1-4320-4124-b985-af27e32d2c36",
   "metadata": {},
   "source": [
    "#### Denoising  \n",
    "is constructing the fit $Y=cX+d$ and then applying the transformation \n",
    "$$\n",
    "\\left[\\begin{array}{c} X \\\\ \\tilde{Y} \\end{array} \\right]\n",
    "= \\left[\\begin{array}{cc} 1 & 0 \\\\ -c &1 \\end{array} \\right]\n",
    "\\left[ \\begin{array}{c} X \\\\ Y\\end{array}  \\right]\n",
    "+\\left[  \\begin{array}{c} 0 \\\\ -d \\end{array}\\right]\n",
    "$$\n",
    "This is exactly finding the average Y value at each X value, and then translating along the $Y$ axis plus shearing along the $Y$ axis in the $T,X$ plane. In the $Y,T$ plane, this just looks like color dependent translation. This denoising translation decreases the variance in heights. The value of  $\\hat{\\tau}$  found after this translation is the same as that before, but the variance of $\\tau$ is smaller and thus the  likelihood of rejecting the null hypothesis is greater. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7136ea-95d3-4a1f-912c-314ac4c4a107",
   "metadata": {},
   "source": [
    "<img src=\"images/denoise.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7664b-447c-4e68-9900-4142543a2af9",
   "metadata": {},
   "source": [
    "After debiasing, $\\tilde{T}\\perp X$.\n",
    "\n",
    "After denoising $\\tilde{Y} \\perp X$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ad24c-80af-40fa-b8e6-f4fb0edb1f5c",
   "metadata": {},
   "source": [
    "**Definition:** The result of these two transformations, debiasing and denoising is <u>orthogonalization</u>\n",
    "\n",
    "ME: If you want to use angles, consider the inner product to be correlation $\\langle A,B\\rangle =\\rho(a,b)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36f6fe7-8197-401f-8d7d-aed642f63bf4",
   "metadata": {},
   "source": [
    "\n",
    "Thus, fitting $\\tilde{Y} = \\tau \\tilde{T}$ is not effected by $X$ because \"$X$ is random\" as far as $\\tilde Y$ and $\\tilde T$ are concerned. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab0968-c19a-42e3-aa88-b439ad4c69d5",
   "metadata": {},
   "source": [
    "## Regression of Potential Outcomes\n",
    "\n",
    "Regression of $Y$ on $X$  over the control population (${Y}_0 = a X + b$) yields $\\hat{Y}_0(x) = \\hat a x + \\hat b$. This function provides a way to impute $Y_0$ on the $T=1$ population; it therefore provides a way to estimate ATT as\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{ATT} & := E[Y_1|T=1] - E[Y_0|T=1] \\\\\n",
    "& \\stackrel{\\text{impute}}{\\approx}  E[Y_1|T=1] -\\frac1{N_1} \\sum\\limits_{i\\in\\{i|T_i=1\\}} (aX_i+b) \\\\\n",
    "&\\stackrel{\\text{emperical}}{=} \\frac1{|\\{i|T_i=1\\}|}\\sum\\limits_{i\\in\\{i|T_i=1\\} }\n",
    "[Y_{i1}- (aX_i+b)]\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948dda76-f04c-4ed7-9f90-b39619f757ea",
   "metadata": {},
   "source": [
    "Regression of $Y$ on $X$ over the treated poulation ${Y}_1 = cX+d$ yields $\\hat{Y}_1 (x)= \\hat c x+ \\hat d$, so how about you average over the whole population to get ATE? \n",
    "$$\n",
    "ATE \\approx \\frac1N \\sum_{i=1}^N [ (cx_i+d) -(ax_i+b)]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e92bbc6-021b-4e49-afa0-86d25e32e481",
   "metadata": {},
   "source": [
    "## Conditionally Random Experiments\n",
    "\n",
    "Work backwards from the fact that we have these regression tools that work when the CIA $(Y_0,Y_1 \\perp T |X)$ holds.\n",
    "\n",
    "We can create experiments where  $(Y_0,Y_1 \\perp T |X)$ by design; let treatment assignment be random for each group with constant $X=x$ using Bernoulli probability of success $p$ to assign treatment with $p: \\{x\\} \\to (0,1)$ dependant only on $x$. ($p(x)$ in the language I have come to detest.) \n",
    "\n",
    "e.g.  Say it is deemed to expensive to run a randomized experiment of giving people credit lines, meaing assigning new credit lines to all people according to the same $p=0.1$ Bernoulli distribution. Then, to perform a conditionally random experiment, you CAN give higher risk people a lower $p=0.01$ value for a Bernoulli distribution, lower risk people a higher $p=0.21$ value for a Bernoulli distribution. Run the experiment, and use CIA tools to reconstruct the ATE, ATT etc. You can bin credit scores in any way you want, and then you have the luxury of knowing exactly how they were binned, as opposed to the forwards version! e.g. You can make $p$ a function of only credit score rounded to the nearest hundred.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52a990-72c8-4c32-8c48-8e8603796121",
   "metadata": {},
   "source": [
    "Be careful! Overlap in treatment groups is needed! Dont assign 100% of the people in some group the same treatment! That violates the common support assumption (aka positivity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca74568a-b41c-43b4-92ca-a7df25763e0e",
   "metadata": {},
   "source": [
    "### Dummy Variables for Variable Intecept\n",
    "\n",
    "Say that you run a contitionally random experiment, or that you otherwise have a categorical confounding variable $G$. (Thus there will be no $X$.) One hot encode to get features $G_j$. You are going to fit\n",
    "$$\n",
    "Y=\\tau T +\\beta_0+\\theta_j G^j\n",
    "$$\n",
    "While I am comfortable thinking about $\\theta_j$ as a slope in going from the ommitted group to the group $j$, another way of thinking is that you get a differnt intercept $\\beta_0 + \\theta_i$ in $T$ vs $Y$ for each of the $G_i$. \n",
    "\n",
    "<img src=\"images/fit_by_group.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8813e3-8c33-4cec-a854-867780a43425",
   "metadata": {},
   "source": [
    "### Saturated Regression Model: Dummies for Variable Slope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec544a2-307b-485f-9587-c0f91780206e",
   "metadata": {},
   "source": [
    "By contrast, in the model\n",
    "$$\n",
    "Y=\\beta_0+ \\theta_j T G^j\n",
    "$$\n",
    "the parameter $\\theta_i$ is the rate of change of $Y$ WRT $T$ for bucket $j$. \n",
    "\n",
    "In patsy formula syntax, the $:$ operator will give you that. By contrast, the $*$ operator will give \n",
    "$$\n",
    "Y=\\beta_0 + \\beta_j  G^j+ \\theta_j T G^j\n",
    "$$\n",
    "so that there is a different intercept $\\beta_0 + \\beta_j$ for each category, as seen in in the output below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0d399c-5139-4f0f-aeab-5da26f103635",
   "metadata": {},
   "source": [
    "```python\n",
    "model = smf.ols(\"default ~ credit_limit * C(credit_score1_buckets)\",\n",
    "                             data=risk_data_rnd).fit()\n",
    "```\n",
    "\n",
    "<img src=\"images/bucket_affines.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6dc345-f5fe-4245-b745-5837411aa746",
   "metadata": {},
   "source": [
    "<img src=\"images/saturated_regression.png\" width=\"300\">\n",
    "\n",
    "The slope called `credit_limit` here is the slope for the group omitted in one hot encoding. In the image, the expreme slope is likely from a small sample size allowing large variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f047a1-6d73-45e4-80d2-888cbb2bf020",
   "metadata": {},
   "source": [
    "Now, each fitted line has differnt intercept and different slope. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56515815-de8b-481f-bc50-141ff2efeaf9",
   "metadata": {},
   "source": [
    "The ATE of credit limit on default rate is the weighted average of these slopes weighted by number of samples (Facure says, but I am worried about weighting by variance being the correct way to get an unbiased ATE  estimate). \n",
    "\n",
    "Presumably this is called saturated regression. \n",
    "\n",
    "ChatGPT says \"saturated regression refers to a modeling approach that includes all possible covariates or predictors in a regression model.\" I objected that since the product of two features is a feature, there are infinitely many features.... so the defnition it gives is nonsesense. It said, that is correct, but reserachers must decide which are important. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90525dda-90ed-4923-bbb1-6a338478a81c",
   "metadata": {},
   "source": [
    "### Weighted Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e13e1-911c-4a9f-b1a3-a81dc66b6818",
   "metadata": {},
   "source": [
    "This is my best guess of what they are trying to say: \n",
    "\n",
    "$\\hat{\\tau}$ from \n",
    "$$\n",
    "Y=\\beta_0 +\\tau T \\\\\n",
    "$$\n",
    "is equal to the variance weighted average over $\\{\\hat{\\tau} + \\hat{\\theta}_j | j =1,...\\}$ from\n",
    "$$\n",
    "Y=\\tau T +\\beta_0+\\theta_i G^j\n",
    "$$\n",
    "and equal to sample size weighted average over $\\{ \\hat{\\theta}_j| j=1,... \\}$ in \n",
    "$$\n",
    "Y=\\beta_0+ \\theta_i T G_j .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d99036-4320-4848-8018-691bb841fd1f",
   "metadata": {},
   "source": [
    "Remember that \n",
    "$$\n",
    "\\hat{\\tau} = \\frac{\\text{Cov}(Y,T)}{\\text{Var}(T)}\n",
    "=\\frac\n",
    "{\\mathbb{E}[(T-\\bar{T})(Y-\\bar{Y})]}\n",
    "{\\mathbb{E} [ (T-\\bar{T})^2]}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50f3b37-1b07-4ffe-a789-ff07c9357906",
   "metadata": {},
   "source": [
    "This difference is at the heart of the following:\n",
    "\n",
    "\"In 2020, the econometric field went through a renaissance regarding the diff-in- diff method\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82c26637-08e4-4947-9743-1fc827157c84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 5)\n",
      "(50000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_data = pd.read_csv(\"./causal-inference-in-python/data/risk_data.csv\")\n",
    "X_cols = [\"credit_limit\", \"wage\", \"credit_score1\", \"credit_score2\"]\n",
    "X = risk_data[X_cols].assign(intercep=1)\n",
    "y = risk_data[\"default\"]\n",
    "\n",
    "print(X.shape), print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ddeb9824-7883-427e-9626-e26189e6a468",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.06252773e-06, -8.82159125e-05, -4.17472814e-05, -3.03928359e-04,\n",
       "        4.03661277e-01])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def regress(y, X): \n",
    "    return np.linalg.inv(X.T.dot(X)).dot(X.T.dot(y))\n",
    "\n",
    "beta = regress(y, X)\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b02557-5594-402e-8fc1-8ab3c3ff923b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d7e634-10ec-42ff-84c5-dc9b89dee4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ab055-0b2b-4ed5-ac92-1ba2031c6898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72583744-e5df-459f-9926-ab917aa7a088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0781e2-be93-4d09-a744-1f53acdc3254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365e6c3-600a-43ba-84e5-3f46e8564241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09a75911-c5f9-40a9-981d-80f9ae5ddb12",
   "metadata": {},
   "source": [
    "---- \n",
    "\n",
    "Example:\n",
    "- continuous T\n",
    "- 2 groups, labeled by $g\\in \\{1,2\\}$. \n",
    "    - Group 1 has \n",
    "        - 1_000 units with \n",
    "        - $\\text{ATE}_1 =1$ expressed as slope WRT T \n",
    "        - variance of $\\mathbb{T}^2 = 1$\n",
    "    - Group 2 has \n",
    "        - 500 units with \n",
    "        - $\\text{ATE}_2 =2$ expressed as slope WRT T \n",
    "        - variance of $\\mathbb{T}^2 = 2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cecc4347-3b9b-4938-80eb-9f9263dbcce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "np.random.seed(123)\n",
    "t1 = np.random.normal(loc=0, scale=1, size=1_000) \n",
    "t2 = np.random.normal(loc=0, scale=2, size=500)\n",
    "df1 = pd.DataFrame(dict(t=t1, y=1.0*t1, g=1))\n",
    "df2 = pd.DataFrame(dict(t=t2, y=2.0*t2, g=2))\n",
    "df = pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e7f0bf-72d3-4521-a767-b9f018b90f6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Regression $Y=\\tau T + \\theta^i G_i +\\beta$ yields $\\hat{\\tau} =1.67$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "bb113067-ee56-4a70-b60a-4a03c1c712fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6681955761233118"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = smf.ols(\"y ~ t + C(g)\", data=df).fit()\n",
    "model.params['t']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3757cca5-fe1f-4803-aff9-68afb3c0d037",
   "metadata": {},
   "source": [
    "Regression $ Y_j = \\tau_j T + \\beta_j$ on each group gives $\\hat{\\tau}_1$ and $\\hat{\\tau}_2$ that you'd think average to $1.67$ via a weighted average by samples size. It does not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "31413adb-f53b-4e70-b73c-ffec9b3462f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3333333333333333"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Regression Y= tau T +beta, slope coefficient by statsmodels\n",
    "def regress(df, t, y):\n",
    "    return smf.ols(f\"{y}~{t}\", data=df).fit().params[t]\n",
    "\n",
    "\n",
    "# Slope for each group \n",
    "effect_by_group = df.groupby(\"g\").apply(regress, y=\"y\", t=\"t\")\n",
    "\n",
    "# average by sample size \n",
    "\n",
    "sizes = df.groupby(\"g\").size()\n",
    "size_weighted_terms = effect_by_group*sizes\n",
    "\n",
    "weighted_average_ate = (size_weighted_terms).sum()/sizes.sum()\n",
    "weighted_average_ate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e33fa3-0301-4710-b50a-bd7cd4a2db07",
   "metadata": {},
   "source": [
    "The question is why $\\hat{\\tau}_1$ and $\\hat{\\tau}_2$ do not average to $1.67$.\n",
    "\n",
    "The answer is that standard deviation of $T$ is the correct weight to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ed232021-8f71-4f17-af74-9f9ac1e67993",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6674426621121978"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def std(y):\n",
    "    return y.std()\n",
    "\n",
    "# pg 130 \n",
    "# \"weights that are proportional to the variance of the treatment in each group.\"\n",
    "weights_of_groups = (df.groupby(\"g\").apply(std)['t'])\n",
    "\n",
    "# average by variance of t for group\n",
    "\n",
    "var_weighted_terms = effect_by_group*(weights_of_groups)\n",
    "var_weighted_average_ate = (var_weighted_terms).sum()/weights_of_groups.sum()\n",
    "var_weighted_average_ate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3999ee-7da4-4027-a261-a3ece2ec31e3",
   "metadata": {},
   "source": [
    "That is, the categorical dummy $Y = \\tau T + \\theta^j G_j + \\beta $ fit was weighting by standard deviation the $\\tau_j$ for the groups by standard deviation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec461fec-6813-4ee6-901d-d3006abf1fb4",
   "metadata": {},
   "source": [
    "### De-Meaning and Fixed Effects\n",
    "\n",
    "There was talk earlier of FWL removing the mean from each bucket. \n",
    "\n",
    "**Definition:** Subtracting the mean of the bucket from each bucket is <u>de-meaning</u>. \n",
    "\n",
    "The python syntax for fitting on de-meaned data follows.\n",
    "```python\n",
    "risk_data_fe = risk_data_rnd.assign( \n",
    "    credit_limit_avg = lambda d: (d.groupby(\"credit_score1_buckets\")\n",
    "                                               [\"credit_limit\"].transform(\"mean\"))\n",
    ")\n",
    "model = smf.ols(\"default ~ I(credit_limit-credit_limit_avg)\",\n",
    "                             data=risk_data_fe).fit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97111cf6-aaf2-4ac6-ae06-5bf05fd6483b",
   "metadata": {},
   "source": [
    "This gives the same $\\hat{\\tau}$ as when we used dummies. \n",
    "\n",
    "The term \"fixed effects\" means controlling for something (like the group outcome mean) that is fixed within a group. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7935c000-3809-4202-a0a4-aeac9df3ee0e",
   "metadata": {},
   "source": [
    "## Omitted Variable Bias: Confounding Through the Lens of Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c169e85-69aa-42c7-816a-031ae54af90d",
   "metadata": {},
   "source": [
    "Regression allows you to be precise about the confounding bias by talking about the long model, the omitted model, and the debiasing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94a0802-e722-4b0c-89c5-c47363e0e39f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{Short model}     &Y = \\tau' T + \\beta^{'}_0\\\\\n",
    "\\text{Long Model}      &Y = \\tau T + \\beta_1 X + \\beta_0\\\\ \n",
    "\\text{Debiasing Model} &X = \\delta_1 T + \\delta_0\\\\            \n",
    "\\end{array}\n",
    "$$\n",
    "    \n",
    "$$\\hat{\\tau}' = \\hat{\\tau} + \\hat{\\beta_1} \\hat{\\delta}_1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e00b39a-3ef2-4690-9428-b8fdc49204e8",
   "metadata": {},
   "source": [
    "Or in garble... The biased estimate of ATE due to an omitted variable is equal to \n",
    "the effect in the model where it is included \n",
    "plus \n",
    "the effect of the omitted variable on the outcome \n",
    "times the regression of omitted on included. (On included? On treatment?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44136eec-29f9-46f1-ad54-13d9c2784e3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neutral Controls\n",
    "\n",
    "What kind of variables should you include in X? \n",
    "\n",
    "Potential problems: common effects (colliders) and mediators  induce selection bias.\n",
    "\n",
    "Priming: there is a bias–variance trade-off when it comes to including certain variables in your regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7cacc9-dfb7-405f-ab90-f2810de68fb0",
   "metadata": {},
   "source": [
    "**Definition:** In causal inference via regression <u>controls</u> are the confounding variables that are included in a regression model.\n",
    "\n",
    "Use of a particular confounding variable might induce more bias in the estimator. In fact, there is a bias-variance tradeoff we need to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0ad495-22fa-4d2e-89f3-00f1d0d69d4b",
   "metadata": {},
   "source": [
    "**Definition:** In causal inference <u>neutral controls</u> are confounding variables that are included in a regression model to reduce bias of the estimated causal relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824dc0ff-6482-4ba8-bab4-eaed8272f815",
   "metadata": {},
   "source": [
    "Demonstrating the need for a differnce calls for an example in code below. \n",
    "\n",
    "#### Bias down, Variance up\n",
    "Note that in the code below $x_3$ is a stronger predictor of $T$ than of $y$ by two orders of magnitude. \n",
    "\n",
    "The estimator $\\hat \\tau$ of the coefficient of $t$ in $Y= \\tau T + \\beta^i x_i + \\beta_0$ and its variance $\\mathbb{V}(\\hat \\tau)$ can be estimated  from the Frisch-Waugh-Lovell debiasing $\\tilde T$ and denoising  $\\tilde Y$, as\n",
    "$$\n",
    "\\hat \\tau = \\frac{\\text{Cov}(\\tilde T,\\tilde Y)}{\\text{var}( \\tilde T)}\n",
    "$$\n",
    "$$\n",
    "\\mathbb{V}(\\hat{\\tau}) = \\frac{ \\text{Var}(\\tilde Y)}{\\text{Var}( \\tilde T) (N-DF)}.\n",
    "$$\n",
    "Since $x_3$ is a stronger predictor of $T$ than of $y$, w anticipate the use of $x_3$ to\n",
    "1. increase  $\\text{Var}(\\hat \\tau)$ by decreasing $\\text{Var}(\\tilde T)$ \n",
    "2. decrease bias to the coefficient of $t$, which is supposed to be 5, by controlling for fewer confounding variables.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "2b6947e4-3109-46a7-9382-1069246ff382",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "se_boost     10.033422\n",
       "bias_diff    -0.012452\n",
       "dtype: float64"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sms\n",
    "\n",
    "def var_bias():\n",
    "    (x1,x2,x3) = (np.random.normal(loc = 0 , scale=1, size = 100) for _ in range(3))\n",
    "    t = np.random.normal(loc =     0.1*x1 + 1*x2 + 10.*x3, scale =1)\n",
    "    y = np.random.normal(loc= 5*t+ 10.*x1 + 1*x2 + 0.1*x3, scale =1)\n",
    "    df = pd.DataFrame( data = dict(t=t,y=y, x1=x1,x2=x2,x3=x3))\n",
    "    nox3 = sms.ols(\"y ~ t + x1 + x2 \", data = df).fit()\n",
    "    withx3 = sms.ols(\"y ~ t + x1 + x2 + x3\", data = df).fit()\n",
    "    se_boost = withx3.bse['t']/nox3.bse['t'] \n",
    "    # bias_boost = (5-withx3.params['t'])/(5- nox3.params['t'])\n",
    "    bias_diff = (withx3.params['t']-5)-(nox3.params['t']-5)\n",
    "\n",
    "    return  se_boost, bias_diff\n",
    "\n",
    "bv_df = pd.DataFrame(columns=['se_boost','bias_diff'])\n",
    "for i in range(100):\n",
    "    bv_df.loc[i] = var_bias()\n",
    "\n",
    "bv_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ff70a-471a-4272-88e2-2fd26c22d7ce",
   "metadata": {},
   "source": [
    "Indeed\n",
    "1. $\\text{Var}(\\hat \\tau)$ goes up by an order of magnitude with use of $x_3$\n",
    "2. bias goes down by a little (a quarter of a percent of the parameter being estimated, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2ea73567-0018-41b5-84a9-488d3be2a2da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24903999999999998"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.012452/5)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413db664-f16d-437b-a0a8-6753cdb18bb9",
   "metadata": {},
   "source": [
    "Side note: The desoising technique CUPED, developed by Microsoft researchers, is very similar to just doing the denoising part of the FWL theorem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326f1aed-f59c-4a3e-b5a7-db5c65b7fe0a",
   "metadata": {},
   "source": [
    "#### Bias up, Variance down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c414fbe4-d868-4d61-87e3-f6cdbb432ab0",
   "metadata": {},
   "source": [
    "Message: Sometimes it is worth accepting a bit of bias in order to reduce variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54f5230-4b00-4ea1-b94c-9e76299b3af9",
   "metadata": {},
   "source": [
    "It’s really hard to have a situation where a covariate causes the treatment but not the outcome. Usually you have multiple covariates, and for a single $X_i$ there are different level of intensity of cause on T and Y, as visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80af9a9f-4e0a-4179-9223-455f0523b08a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T11:02:49.835553Z",
     "start_time": "2023-05-12T11:02:49.631596Z"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"242pt\" height=\"160pt\"\n",
       " viewBox=\"0.00 0.00 242.00 160.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 156)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-156 238,-156 238,4 -4,4\"/>\n",
       "<!-- X1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>X1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-134\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-128.95\" font-family=\"Times,serif\" font-size=\"14.00\">X1</text>\n",
       "</g>\n",
       "<!-- T -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>T</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"117\" cy=\"-91\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"117\" y=\"-85.95\" font-family=\"Times,serif\" font-size=\"14.00\">T</text>\n",
       "</g>\n",
       "<!-- X1&#45;&gt;T -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>X1&#45;&gt;T</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"5\" d=\"M49.3,-123.59C58.53,-119.08 69.59,-113.68 80.03,-108.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"5\" points=\"81.92,-112.52 88.98,-104.2 78.08,-104.66 81.92,-112.52\"/>\n",
       "</g>\n",
       "<!-- Y -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Y</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"207\" cy=\"-68\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"207\" y=\"-62.95\" font-family=\"Times,serif\" font-size=\"14.00\">Y</text>\n",
       "</g>\n",
       "<!-- X1&#45;&gt;Y -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>X1&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.41,-134.33C78.65,-133.73 115.07,-130.5 144,-118 158.62,-111.68 172.64,-100.92 183.64,-91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"185.8,-93.78 190.66,-84.37 180.99,-88.69 185.8,-93.78\"/>\n",
       "</g>\n",
       "<!-- T&#45;&gt;Y -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>T&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M142.51,-84.59C151.12,-82.34 160.97,-79.77 170.3,-77.33\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"171.06,-80.75 179.85,-74.83 169.29,-73.98 171.06,-80.75\"/>\n",
       "</g>\n",
       "<!-- X2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>X2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-77\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-71.95\" font-family=\"Times,serif\" font-size=\"14.00\">X2</text>\n",
       "</g>\n",
       "<!-- X2&#45;&gt;T -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>X2&#45;&gt;T</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M53.45,-81.05C61.41,-82.32 70.36,-83.74 78.94,-85.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"78.38,-88.56 88.8,-86.67 79.48,-81.65 78.38,-88.56\"/>\n",
       "</g>\n",
       "<!-- X2&#45;&gt;Y -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>X2&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M52.55,-70.63C63.87,-68.01 77.53,-65.29 90,-64 116.28,-61.27 146.08,-62.39 168.89,-64.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"168.36,-67.6 178.62,-64.95 168.95,-60.62 168.36,-67.6\"/>\n",
       "</g>\n",
       "<!-- X3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>X3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">X3</text>\n",
       "</g>\n",
       "<!-- X3&#45;&gt;T -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>X3&#45;&gt;T</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M44.55,-31.73C57.5,-42.47 75.72,-57.59 90.53,-69.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"88.13,-72.43 98.06,-76.12 92.6,-67.04 88.13,-72.43\"/>\n",
       "</g>\n",
       "<!-- X3&#45;&gt;Y -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>X3&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"5\" d=\"M52.38,-24.85C81.72,-33.09 131.4,-47.05 166.3,-56.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"5\" points=\"164.82,-60.98 175.63,-59.47 167.18,-52.55 164.82,-60.98\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7faa882f6fa0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz as gr\n",
    "# Order here seems to determine vertical order in the visualization.\n",
    "g = gr.Digraph(graph_attr={\"rankdir\": \"LR\"})\n",
    "g.edge(\"X1\", \"T\", penwidth=\"5\"),\n",
    "g.edge(\"X2\", \"T\", #penwidth=\"3\", \n",
    "       style=\"dotted\"),\n",
    "g.edge(\"X3\", \"T\", penwidth=\"1\"),\n",
    "g.edge(\"X1\", \"Y\", penwidth=\"1\"),\n",
    "g.edge(\"X2\", \"Y\", #penwidth=\"3\", \n",
    "       style = \"dotted\"),\n",
    "g.edge(\"X3\", \"Y\", penwidth=\"5\"),\n",
    "\n",
    "g.edge(\"T\", \"Y\"),\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc3235-bf53-4df2-afea-ebc981f96f8d",
   "metadata": {},
   "source": [
    "$X_1$ explains away variance in $T$ (and thus increases variance in $\\hat{\\tau}$) more than it removes bias. \n",
    "\n",
    "$X_3$ reduces confounder bias more than explaining variance in $T$. \n",
    "\n",
    "Seems to me like $X_1$ is a bad choice for variable to control for, $X_2$ is good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "5ddcbcde-e6de-450c-a61e-e04f9bb3f1c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.2707</td> <td>    0.527</td> <td>    0.514</td> <td> 0.608</td> <td>   -0.775</td> <td>    1.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>t</th>         <td>    0.8664</td> <td>    0.607</td> <td>    1.427</td> <td> 0.157</td> <td>   -0.339</td> <td>    2.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>        <td>   -7.0628</td> <td>    6.038</td> <td>   -1.170</td> <td> 0.245</td> <td>  -19.049</td> <td>    4.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>        <td>    0.0143</td> <td>    3.128</td> <td>    0.005</td> <td> 0.996</td> <td>   -6.195</td> <td>    6.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>        <td>    9.6292</td> <td>    0.887</td> <td>   10.861</td> <td> 0.000</td> <td>    7.869</td> <td>   11.389</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\toprule\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &       0.2707  &        0.527     &     0.514  &         0.608        &       -0.775    &        1.316     \\\\\n",
       "\\textbf{t}         &       0.8664  &        0.607     &     1.427  &         0.157        &       -0.339    &        2.072     \\\\\n",
       "\\textbf{x1}        &      -7.0628  &        6.038     &    -1.170  &         0.245        &      -19.049    &        4.923     \\\\\n",
       "\\textbf{x2}        &       0.0143  &        3.128     &     0.005  &         0.996        &       -6.195    &        6.224     \\\\\n",
       "\\textbf{x3}        &       9.6292  &        0.887     &    10.861  &         0.000        &        7.869    &       11.389     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "n = 100\n",
    "(x1, x2, x3) = (np.random.normal(0, 1, n) for _ in range(3)) \n",
    "t = np.random.normal(10*x1 + 5*x2 + x3)\n",
    "# ate = 0.05\n",
    "y = np.random.normal(0.05*t + x1 + 5*x2 + 10*x3, 5)\n",
    "df = pd.DataFrame(dict(y=y, t=t, x1=x1, x2=x2, x3=x3))\n",
    "smf.ols(\"y~t+x1+x2+x3\", data=df).fit().summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3cf69-989c-478f-acb4-5965fccf7a90",
   "metadata": {},
   "source": [
    "The 95% CI contains 0 so we can not reject $H_0: \\tau =0$. This is because the variance in $T$ is too big because $X_1$ is explaining away variance in $T$ and thus increasing variance in $\\hat{\\tau}$.\n",
    "\n",
    "Removing $X_1$ from the fit, the 95% CI fot $\\hat{\\tau}$ no longer contains 0 and thus the null hypothesis can be rejected.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "441ab8f5-4a8c-4a21-aa07-2d23eca95c1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T11:02:49.864735Z",
     "start_time": "2023-05-12T11:02:49.854195Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.1889</td> <td>    0.523</td> <td>    0.361</td> <td> 0.719</td> <td>   -0.849</td> <td>    1.227</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>T</th>         <td>    0.1585</td> <td>    0.046</td> <td>    3.410</td> <td> 0.001</td> <td>    0.066</td> <td>    0.251</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>        <td>    3.6095</td> <td>    0.582</td> <td>    6.197</td> <td> 0.000</td> <td>    2.453</td> <td>    4.766</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>        <td>   10.4549</td> <td>    0.537</td> <td>   19.453</td> <td> 0.000</td> <td>    9.388</td> <td>   11.522</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\toprule\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &       0.1889  &        0.523     &     0.361  &         0.719        &       -0.849    &        1.227     \\\\\n",
       "\\textbf{T}         &       0.1585  &        0.046     &     3.410  &         0.001        &        0.066    &        0.251     \\\\\n",
       "\\textbf{x2}        &       3.6095  &        0.582     &     6.197  &         0.000        &        2.453    &        4.766     \\\\\n",
       "\\textbf{x3}        &      10.4549  &        0.537     &    19.453  &         0.000        &        9.388    &       11.522     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smf.ols(\"Y~T+x2+x3\", data=df).fit().summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16ee4ec-6a1c-4a6a-aaeb-0e9aa2d35203",
   "metadata": {},
   "source": [
    "So, if a confounding factor explains the treatment too much and almost nothing about the outcome, you should really consider dropping it; it is not worth the increase in variance to have a small decrease in bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c46aa0-ba44-46db-b58c-c9381ec0737f",
   "metadata": {},
   "source": [
    "Exactly how weak the confounder should be in terms of explaining the treatment to justify removing it is still an open question in causal inference research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04464589-09a4-49d0-b115-11d201dd80fd",
   "metadata": {},
   "source": [
    "# 5 Propensity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf7830-8e23-45e2-a1bd-226302b9506a",
   "metadata": {},
   "source": [
    "In orthogonalization we build models of $T$ and $Y$ to obtain residuals to debias and denoise. \n",
    "\n",
    "In propensity weighting, we build models of treatment assignment and uses the model’s prediction to reweight the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86965620-ac4c-42c7-9459-8b23fd447bd0",
   "metadata": {},
   "source": [
    "**Definition:** A <u>propensity score</u>  $e(x)$ is an estimator of $P( T |X=x)$.\n",
    "\n",
    "In the binary treatment case, propensity score estimates $E(T|X=x)$ \n",
    "\n",
    "You can simply condition on the propensity score in order to block the backdoor paths that flow through X.\n",
    "\n",
    "<img src=\"images/propensity_graph.svg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac291517-5d5d-42e1-aa43-81e412bdfaa0",
   "metadata": {},
   "source": [
    "Think about it: if $i$ and $j$ have the exact same probability of receiving the treatment, the only reason one of them did and the other didn’t is pure chance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5af25-6703-49fc-8af3-e85b1dcc7599",
   "metadata": {},
   "source": [
    "In a (perfectly executed) conditional experiment you would havbe the true $P(T|X)$. In an observational study you need to estimate it with some $e(x)$. \n",
    "\n",
    "For binary treatment, logistic regression is used. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800468f6-f9fd-4801-a9ef-329624418a22",
   "metadata": {},
   "source": [
    "Recall that OLS estimates $T$ as a function of $X$ in the debiasing phase. That is, it estimates $E[T|X=x]$. But a linear estimate of binary outcome  vs continuous $x$ is pretty bad. \n",
    "\n",
    "Fitting the logistic regression for a propensity score\n",
    "$$\n",
    "e = \\sigma(\\beta_1 X + \\beta_0)\n",
    "$$\n",
    "and then \n",
    "$$\n",
    "Y = \\tau T + \\delta e\n",
    "$$\n",
    "has an effect on $\\hat{\\tau}$ very similar to debiasing. The difference; OLS modeled $T(X)$ linearly, propensity score modeled it logistically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c80329-90a8-40c9-8082-48a0b1b6e2c5",
   "metadata": {},
   "source": [
    "Propensity score is a dimensionality reduction to one dimension; it thus provides a notion of distance. \n",
    "\n",
    "In propensity score matching you look for nearest neighbors of each unit in the other treatment group, and average over treatment effect differences. The use of KNN here is fround upon by the author; \n",
    "- it is difficult to derive the variance of the resulting $\\hat{\\tau}$.\n",
    "- inefficiency of kNN in high dimension when conditioning on $e$ is not a good replacement for the problem of dimsnsionality when using $X$. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3105df-b5dc-470a-a215-bb3f52c65cc8",
   "metadata": {},
   "source": [
    "See the paper  “Why Propensity Scores Should Not Be Used for Matching,” by King and Nielsen\n",
    "\n",
    "### Inverse Propensity Weighting\n",
    "Here is a novel way to account for confounding bias; $X$ causes $T$ and $Y$, and we need to close that back door of association. \n",
    "\n",
    "If $i$ was treated but $e(x_i) \\approx 0 $ then $i$ looks a lot like the untreated.  \n",
    "\n",
    "Thus, $Y_i$ provides an estimate of the counterfactual  $E[Y_1| T = 0, X=x_i]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d6038-836b-49ed-9086-53c457c709b7",
   "metadata": {},
   "source": [
    "We'd like to use many of these interesting individuals. We'd also like to give more weight lower the $e$, since they look like they are in the other group. \n",
    "\n",
    "The weighting \n",
    "$$E{[Y_t]} := E\\left[ \\frac{\\mathbb{1}(T=t)Y}{P[T=t|X=x]} \\right]$$ \n",
    "makes treatment appear to have been randomly weighted.\n",
    "\n",
    "In the case of binary treatment this reads\n",
    "$$\n",
    "E{[Y_1]} = E\\left[ \\frac{\\mathbb{1}(T=1)Y}{P[T=1|X=x]} \\right] \n",
    "\\stackrel{\\text{model } e}{\\approx} E\\left[ \\frac{\\mathbb{1}(T=1)Y}{ e(x)}\\right] \n",
    "\\stackrel{\\text{emperical}}{\\approx} \\frac1{\\vert\\{i|T_i=1\\}\\vert} \\sum\\limits_{i\\in \\{i|T_i=1\\}} \\frac{Y_i}{e(x_i)}\n",
    "\\\\ \n",
    "E{[Y_0]} = E\\left[ \\frac{\\mathbb{1}(T=0)Y}{P[T=0|X=x]} \\right] \n",
    "\\stackrel{\\text{model } e}{\\approx} E\\left[ \\frac{\\mathbb{1}(T=1)Y}{1- e(x)}\\right] \n",
    "\\stackrel{\\text{emperical}}{\\approx} \\frac1{\\vert \\{i|T_i=0\\} \\vert} \\sum\\limits_{i\\in \\{i|T_i=0\\}} \\frac{Y_i}{1-e(x_i)}\n",
    "\\\\ \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633249d3-2fa8-45a7-93df-68b677bebfb2",
   "metadata": {},
   "source": [
    "This provides the estimate\n",
    "$$\n",
    "\\text{ATE} = E{[Y_1]} - E[Y_0].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f982093-5214-4763-9739-21cfa3b4a99a",
   "metadata": {},
   "source": [
    "In the plot below, the size of the marker indicates weight. You see that heavy weight is given to \n",
    "- the treated with a low (predicted) probability of treatment (large triangles)\n",
    "- the untreated with high probability of treatment (large circles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9872ae8-f858-4ec0-8a5f-6c9d0fc1abd1",
   "metadata": {},
   "source": [
    "<img src=\"images/IPW.png\" width=\"300\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ead37f-940e-457a-b771-cf14ac0dfc56",
   "metadata": {},
   "source": [
    "High weight to \n",
    "- the treated who look untreated, \n",
    "- the untreated who look treated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa67f0a-0a68-401a-8929-dca858a84f79",
   "metadata": {},
   "source": [
    "---\n",
    "#### Confusions:\n",
    "\n",
    "Discussion on modeling: There is no training of a model to predict $Y$ in IPW. The only model is $e$. That seems magical; we get ATE modeiling only $e$. \n",
    "\n",
    "Discussion on $X$ conditioning; I keep thinking that this is giving a $X$ conditioned ATE... but it is not; Those expectation values have estimators that are sums over all units $i$.\n",
    "\n",
    "\n",
    "Discussion on \"pseudo-population\" \n",
    "\n",
    "I'm struggling with this phrase;\n",
    "\n",
    "The quantity\n",
    "$$E{[Y_t]} := E\\left[ \\frac{\\mathbb{1}(T=t)Y}{P[T=t|X=x]} \\right]$$ \n",
    "creates a pseudo-population that approximates what would have happened if everyone had received the treatment $t$.\n",
    "\n",
    "Well... its emperical estimator is a average over units who did recieve treatmet $t$.... and the quantity averaged is an inflated effect $Y_i/e(x_i)$. That is hardly \"everyone recieved the treatment $t$.\" Those who were unlikely to recieve the treatment $t$ were given high inflation. \n",
    "\n",
    "REally, I'm just wondering how the effect of this inflation vanishes; these quantities seem like they are just bigger than the terms in ATE. I guess, in the end, the way to see that they wash out is with the algebra revelations below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facb9985-5791-4311-b6e3-27a8c5dc4c8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fed6e9f-5ca5-462c-b498-4e3a23efdf80",
   "metadata": {},
   "source": [
    "#### Python e.g.\n",
    "We will want to see if an intervention like manager training increases a manager's engagement with underlings as measured by $Y=$`engagement_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8039ac6e-7d63-4c31-9818-3ae636f8544f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>departament_id</th>\n",
       "      <th>intervention</th>\n",
       "      <th>engagement_score</th>\n",
       "      <th>tenure</th>\n",
       "      <th>n_of_reports</th>\n",
       "      <th>gender</th>\n",
       "      <th>role</th>\n",
       "      <th>last_engagement_score</th>\n",
       "      <th>department_score</th>\n",
       "      <th>department_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>0.277359</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.614261</td>\n",
       "      <td>0.224077</td>\n",
       "      <td>843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.449646</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.069636</td>\n",
       "      <td>0.224077</td>\n",
       "      <td>843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>0.769703</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.866918</td>\n",
       "      <td>0.224077</td>\n",
       "      <td>843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.121763</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.029071</td>\n",
       "      <td>0.224077</td>\n",
       "      <td>843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>1.526147</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.589857</td>\n",
       "      <td>0.224077</td>\n",
       "      <td>843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   departament_id  intervention  engagement_score  tenure  n_of_reports  \\\n",
       "0              76             1          0.277359       6             4   \n",
       "1              76             1         -0.449646       4             8   \n",
       "2              76             1          0.769703       6             4   \n",
       "3              76             1         -0.121763       6             4   \n",
       "4              76             1          1.526147       6             4   \n",
       "\n",
       "   gender  role  last_engagement_score  department_score  department_size  \n",
       "0       2     4               0.614261          0.224077              843  \n",
       "1       2     4               0.069636          0.224077              843  \n",
       "2       2     4               0.866918          0.224077              843  \n",
       "3       2     4               0.029071          0.224077              843  \n",
       "4       1     4               0.589857          0.224077              843  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "df = pd.read_csv(\"./causal-inference-in-python/data/management_training.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb614a-347f-43c5-9fb2-218469d6c5bc",
   "metadata": {},
   "source": [
    "We start by building a model of the probability of intervention as a function of some of the other features. This is a propensity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d6b2c7c-7497-4ce5-8960-a92f34fe54f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "ps_model = smf.logit(\"\"\"intervention ~\n",
    "            tenure + last_engagement_score + department_score\n",
    "            + C(n_of_reports) + C(gender) + C(role)\"\"\", data=df).fit(disp=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc5aa63-9d7d-46cd-b554-643f97a1f130",
   "metadata": {},
   "source": [
    "With this, we assign a propensity score to each manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c8518e7-95a7-4e76-9fee-c3254a53e0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intervention</th>\n",
       "      <th>engagement_score</th>\n",
       "      <th>propensity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.277359</td>\n",
       "      <td>0.596106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.449646</td>\n",
       "      <td>0.391138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.769703</td>\n",
       "      <td>0.602578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.121763</td>\n",
       "      <td>0.580990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.526147</td>\n",
       "      <td>0.619976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   intervention  engagement_score  propensity_score\n",
       "0             1          0.277359          0.596106\n",
       "1             1         -0.449646          0.391138\n",
       "2             1          0.769703          0.602578\n",
       "3             1         -0.121763          0.580990\n",
       "4             1          1.526147          0.619976"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ps = df.assign(propensity_score = ps_model.predict(df))\n",
    "data_ps[[\"intervention\", \"engagement_score\", \"propensity_score\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3c585f-6f52-4ade-b53e-7702c2147507",
   "metadata": {},
   "source": [
    "Calculate inverse propensity weight (IPW) to go with each engagement score ($Y$), and the inverse propensity score weighted $Y_i$s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e9401e-8930-4b00-a7e9-f7ef0f03cc99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_t = 1/data_ps.query(\"intervention==1\")[\"propensity_score\"]\n",
    "weight_nt = 1/(1-data_ps.query(\"intervention==0\")[\"propensity_score\"])\n",
    "# WTF... these are effects, they are Ys. \n",
    "y1s = data_ps.query(\"intervention==1\")[\"engagement_score\"]\n",
    "y0s = data_ps.query(\"intervention==0\")[\"engagement_score\"]\n",
    "ipw_y1s = y1s*weight_t\n",
    "ipw_y0s = y0s*weight_nt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f4c7e6-5971-447c-8776-91073cd2c863",
   "metadata": {},
   "source": [
    "We then calculate the mean over units in the treatment groups; This is \n",
    "- not a weighted average with inverse propensity scores as weights inverse propensity score, is it \n",
    "- the mean of weighted effects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18fa768e-e53a-4359-a6a4-29cd3bcc4928",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[Y1]: 0.11656317232946731\n",
      "E[Y0]: -0.14941553647814407\n",
      "ATE 0.2659787088076114\n"
     ]
    }
   ],
   "source": [
    "Ey1 = sum(ipw_y1s)/len(data_ps)\n",
    "Ey0 = sum(ipw_y0s)/len(data_ps)\n",
    "print(\"E[Y1]:\", Ey1)\n",
    "print(\"E[Y0]:\", Ey0)\n",
    "print(\"ATE\", Ey1 - Ey0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812a81b9-054c-4daf-96d9-4cd6b0fbd1c7",
   "metadata": {},
   "source": [
    "### Algebra Revelations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62384890-8e2d-4d02-ae18-24ccf329657d",
   "metadata": {},
   "source": [
    "With binary $T$, the estimator of ATE for IPW is \n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\hat{\\tau}_{\\text{IPW}} &= \n",
    "E\\left[   \n",
    "\\frac{ \\mathbb{1}(T=1)Y }\n",
    "{P(T=1|X)}  \n",
    "\\right]\n",
    "-\n",
    "E\\left[   \n",
    "\\frac{ \\mathbb{1}(T=0 )Y }\n",
    "{P(T=0|X)}  \n",
    "\\right]\\\\\n",
    "& =E\\left[ \n",
    "Y\\frac{T-e(x)}{e(x)(1-e(x))}\n",
    "\\right]\\\\\n",
    "& =E\\left[ \n",
    "\\frac{Y(T-E(T|X))}{\\mathbb{V}(T|X))}\n",
    "\\right]\\\\\n",
    "& = \n",
    "\\frac{E[Y(T-E(T|X))]}{\\mathbb{V}(T|X))}\n",
    "\\\\\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ca512-21f8-4777-b2bf-f7b78696a933",
   "metadata": {},
   "source": [
    "where I have used that the variance of a Bernoulli distribution with $p=e(x)$ is $p(1-p)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da3b15-9693-4fa9-a30f-e20f2d159b62",
   "metadata": {},
   "source": [
    "Note that is is very similar to what OLS gives as an estimate:\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\hat{\\tau}_{ols}& = \n",
    "\\frac{E(Y(T-E(T|X)))}{ E (\\mathbb{V}(T|X)))}\\\\\n",
    "&=\n",
    "\\frac{E(Y(T-E(T|X)))}{ \\mathbb{V}(T|X))} \\frac{\\mathbb{V}(T|X))}{E [\\mathbb{V}(T|X))]}\\\\\n",
    "&=\\hat{\\tau}_{\\text{IPW}} \\frac{\\mathbb{V}(T|X))}{E [\\mathbb{V}(T|X))]}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96ac138-3356-479a-b687-7e44e0eb4eaa",
   "metadata": {},
   "source": [
    "IPW weights each sample by 1, while OLS weights the group effects by the conditional treatment variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279905ac-26fa-4132-82dd-32ef698b6d37",
   "metadata": {},
   "source": [
    "Standard error for IPW is calculated with the bootstrap method; repeatedly\n",
    "- resample data with replacement, \n",
    "- use the sample generate $\\hat{\\tau}_{\\text{IPW}}$  \n",
    "\n",
    "Then generate a histogram of $\\hat{\\tau}_{\\text{IPW}}$  values, normalize it for a PDF, find the 2.5th and 97.5th percentile, there is your confidence interval. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a352447-f4bd-4765-be20-371f34f2281a",
   "metadata": {},
   "source": [
    "The section on Stabilized Propensity Weights simply advises weighting by $\\frac{P(T=t)}{P(T+t|X)}$ and notes the analogy with a common data science technique called \"importance sampling\". I also glazed over pseudo population ideas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b9b85e-4f7e-4125-a6f6-b87ce8d38e43",
   "metadata": {},
   "source": [
    "### Propensity score for Selection Bias\n",
    "\n",
    "You saw how to use propensity score weighting as a way to account and control for confounding bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fb06e-ab4f-4c3a-8731-20cd36fade9f",
   "metadata": {},
   "source": [
    "For a second time... I do not have the oomph here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b05a0bf-429c-4f00-8baa-b83339e0a7f2",
   "metadata": {},
   "source": [
    "### Design\n",
    "which one should you use and when? Regression or IPW?\n",
    "\n",
    "**Definition:** <u>Model-based identification</u> involves making assumptions in the form of a model of the potential outcomes conditioned on the treatment and additional covariates.\n",
    "\n",
    "**Definition:** <u>design-based identification</u> is all about making assumptions about the treatment assignment mechanism. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d15e28f-13ad-4101-88d0-87c286ffc68a",
   "metadata": {},
   "source": [
    "Frisch-Waugh-Lovell is both; it is \n",
    "- designed based because you model $T$ as a function of $X$ (debiasing) \n",
    "- model based because you model $Y$ as a function of $\\tilde{T}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d33b85-5d1d-4c4a-a384-9526a711bd20",
   "metadata": {},
   "source": [
    "IPW is purely design based; it is about modeling $T$ as a logistic function of $X$. \n",
    "\n",
    "Later we will learn about Synthetic Control, which is purely model-based.\n",
    "\n",
    "ask yourself which type of assumption you are more comfortable with, that \n",
    "- you can model the treatment assignment \n",
    "- you can model the potential outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bd7920-ea73-467b-b1db-6d0c65e0f7e3",
   "metadata": {},
   "source": [
    "## Doubly Robust Estimation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b74476-0860-486f-a492-9f3bbf8da36a",
   "metadata": {},
   "source": [
    "There is a slick way to simultaneously try a model based identification and a design based identification; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670f52b9-7f49-4140-a2a9-b61192d33062",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "570580b8-285a-4fad-8f42-88c761f1b4a6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "623eca7f-99d3-4216-94dd-b56843598d1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6f61ca2-54cc-492f-a3bd-778219291845",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7336b0d-e398-4fe8-bf9a-8a986d035350",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "089b9d28-7f7b-4942-8efa-feaf9620a13f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0fc6ace-fdba-4495-93eb-af47238cc358",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fad60334-0e5c-4f87-b125-d3015da01d3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8084b75-97f3-4914-a754-5b61c0db8376",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6b81f9a-e019-4248-aaef-8bc59461c78e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0185df08-cbf4-4bbf-8335-ae20db0f923b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c629c243-7c81-48ec-a66f-7c3eb8a616c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c224d3ff-191c-4d1f-8e5b-014828ae3db2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f318a35-960e-401a-b265-d7d390d4db0a",
   "metadata": {},
   "source": [
    "## Continuous treatment\n",
    "\n",
    "Continuous treatments are way more complicated to deal with. So much so that I would say that causal inference as a science doesn’t have a very good answer on how to deal with them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581335ee-8a2f-44d9-883d-16d2f4963333",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2482be6-c2ff-436f-9865-e3ed37a3d87b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07a10cbe-c5d4-4255-9f5c-3c5b40f223ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "207a774a-2faa-4f4e-b03e-d8639a65d711",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f93249d1-495e-48b8-8e84-d536821ec55f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f12b556-20b5-4799-bd85-65261ac49339",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a31555a-9e56-4c8d-9fa4-b30d672d8087",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13e73ffe-c6c0-4c38-9299-c37318225c7a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50a24bf1-27fb-44d4-b9e0-d8c281eb65a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4847093b-5d25-4675-91e4-ccc01a98a2a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f52f81cd-9456-49c7-99bd-b6327ff4a337",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd736ca3-0fce-42b2-9736-90c8e99829f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d462b65c-e003-41fd-80a2-1c20228a0991",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "654c20bf-15d3-4c4f-8d53-841f38d80a47",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9b6d17f-8873-4185-8638-d2f88f0aebb0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ad717e7-c79f-41fc-8a13-d83b4e9c366d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
